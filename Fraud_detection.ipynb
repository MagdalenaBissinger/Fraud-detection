{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Synthetic Financial Datasets For Fraud Detection (kaggle.com) - https://www.kaggle.com/datasets/ealaxi/paysim1"
      ],
      "metadata": {
        "id": "2dEMrqe8Q544"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkjsipDMUND6",
        "outputId": "fcb1dd69-f9e9-4aad-9931-a92601697bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=081009cca3fecfbbd83e8f6b76b93ba2abff3b401cf6cde2cf780ef3e0dfb6c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/6th semester Bilbao/Big Data/PS_20174392719_1491204439457_log.csv\""
      ],
      "metadata": {
        "id": "qrjOj47mUfW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "j0FQIkCyUp0h",
        "outputId": "0548f6e3-11d1-4440-80b8-2e193fab7cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'findspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b3889db9875a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv(f'{path}', header=True, quote='\"', escape='\"', multiLine=True)"
      ],
      "metadata": {
        "id": "LAK4nFaWU4nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(data.show(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "JM1qWefwVFk2",
        "outputId": "03502472-7b4b-4e1f-ca4d-819c1096e90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
            "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
            "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
            "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data"
      ],
      "metadata": {
        "id": "RJy2LfNnWPa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - TRANSACTION VOLUME OVER TIME - calculate the total number of transactions (grouped by type) over each time step (1 hour)"
      ],
      "metadata": {
        "id": "EPDD-y6xWJ-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Group by 'step' and 'type', then count the number of transactions in each group\n",
        "transaction_volume_over_time = df.groupBy(\"step\", \"type\").count()\n",
        "\n",
        "# Show the results\n",
        "transaction_volume_over_time.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNh5LZ3hWLQo",
        "outputId": "39bb7b81-78e0-43dd-ec8a-6d521bf90592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+-----+\n",
            "|step|    type|count|\n",
            "+----+--------+-----+\n",
            "|  25| CASH_IN|  276|\n",
            "|  37|CASH_OUT|13131|\n",
            "| 105|CASH_OUT|    9|\n",
            "| 107|CASH_OUT|    5|\n",
            "| 157| CASH_IN| 6851|\n",
            "+----+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - COUNT FRAUDULENT TRANSACTION  BY TYPE OF TRANSACTION - count the number of transitions, which are tagged as fraud and grouped by type of transaction"
      ],
      "metadata": {
        "id": "MPA3uSM-WYAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Filter for fraudulent transactions\n",
        "fraud_transactions = df.filter(df.isFraud == 1)\n",
        "\n",
        "# Group by 'type' and count the number of fraudulent transactions for each type\n",
        "fraud_count_by_type = fraud_transactions.groupBy(\"type\").count()\n",
        "\n",
        "# Show the results\n",
        "fraud_count_by_type.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4iOKkMsWZIk",
        "outputId": "82f1bc6f-7d72-488d-a51d-44e88e676b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|    type|count|\n",
            "+--------+-----+\n",
            "|TRANSFER| 4097|\n",
            "|CASH_OUT| 4116|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - MEAN TRANSACTION VALUE FOR ONLY FRAUDULENT TRANSACTIONS - calculate the average of the transaction, which are tagged as fraud"
      ],
      "metadata": {
        "id": "tI1E-sYGWieB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Filter for fraudulent transactions\n",
        "fraud_transactions = df.filter(df.isFraud == 1)\n",
        "\n",
        "# Calculate the average amount of fraudulent transactions\n",
        "average_fraud_transaction_value = fraud_transactions.agg(F.mean(\"amount\").alias(\"avg_fraud_amount\"))\n",
        "\n",
        "# Show the results\n",
        "average_fraud_transaction_value.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qh_dymuWg11",
        "outputId": "05eba334-d9ac-4954-cd1b-8894f8c36c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "| avg_fraud_amount|\n",
            "+-----------------+\n",
            "|1467967.299140387|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - TOP CUSTOMERS BY TOTAL AMOUNT OF ALL THEIR TRANSACTIONS - find the top 10 customers by calculating the sum of every transaction grouped by nameOrig"
      ],
      "metadata": {
        "id": "GNFgughfXCd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Group by 'nameOrig', sum the 'amount', and order by the sum in descending order\n",
        "top_customers_by_total_amount = df.groupBy(\"nameOrig\").agg(F.sum(\"amount\").alias(\"total_amount\")).orderBy(F.desc(\"total_amount\"))\n",
        "\n",
        "# Show the top 10 customers\n",
        "top_customers_by_total_amount.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOPeGQZyXBiu",
        "outputId": "b6fd47c5-ccee-4072-b7a1-cb1157d0c64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|   nameOrig| total_amount|\n",
            "+-----------+-------------+\n",
            "|C1715283297|9.244551664E7|\n",
            "|C2127282686|7.382349036E7|\n",
            "|C2044643633|7.117248042E7|\n",
            "|C1425667947| 6.98867313E7|\n",
            "|C1584456031|6.933731627E7|\n",
            "| C811810230|6.750076129E7|\n",
            "| C420748282|6.676127221E7|\n",
            "|C1139847449|6.423444819E7|\n",
            "| C300140823|6.384799258E7|\n",
            "| C372535854|6.329483963E7|\n",
            "+-----------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - LARGE TRANSACTIONS, WHICH ARE NOT DESCRIBED AS FRAUD - find transactions with large amounts transferred, which are not flagged as fraud"
      ],
      "metadata": {
        "id": "0MxpdHdxXyeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Define a threshold for what you consider a large transaction\n",
        "large_amount_threshold = 200000\n",
        "\n",
        "# Filter for transactions with large amounts that are not marked as fraud\n",
        "large_non_fraud_transactions = df.filter((df.amount > large_amount_threshold) & (df.isFraud == 0))\n",
        "\n",
        "# Select relevant columns and show some of the results\n",
        "large_non_fraud_transactions.select(\"type\", \"amount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H4NSuHWXvyQ",
        "outputId": "0ec55c0e-e98b-48c3-ded8-613f9014063b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|    type|    amount|\n",
            "+--------+----------+\n",
            "|CASH_OUT| 229133.94|\n",
            "|TRANSFER|  215310.3|\n",
            "|TRANSFER| 311685.89|\n",
            "|TRANSFER| 224606.64|\n",
            "|TRANSFER| 379856.23|\n",
            "|TRANSFER|1505626.01|\n",
            "|TRANSFER| 554026.99|\n",
            "|TRANSFER| 761507.39|\n",
            "|TRANSFER|1429051.47|\n",
            "|TRANSFER| 358831.92|\n",
            "|TRANSFER|  367768.4|\n",
            "|TRANSFER| 209711.11|\n",
            "|TRANSFER| 583848.46|\n",
            "|TRANSFER|1724887.05|\n",
            "|TRANSFER| 710544.77|\n",
            "|TRANSFER| 581294.26|\n",
            "|CASH_OUT| 212228.35|\n",
            "|CASH_OUT|  419801.4|\n",
            "|CASH_OUT| 335416.51|\n",
            "|TRANSFER| 330757.04|\n",
            "+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MACHINE LEARNING ALG"
      ],
      "metadata": {
        "id": "LmMqgAKyY8CY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chat.openai.com/share/cac93b3c-3d82-4e79-9b2a-d46be2f8423a"
      ],
      "metadata": {
        "id": "J2m2z3qLYpc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WsQmUCCwGfz",
        "outputId": "5a63461f-6cf0-4fe8-ba5a-0ed656e36af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
            "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
            "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
            "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 7817.71|  C90045638|      53860.0|      46042.29| M573487274|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 7107.77| C154988899|     183195.0|     176087.23| M408069119|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 7861.64|C1912850431|    176087.23|     168225.59| M633326333|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 4024.36|C1265012928|       2671.0|           0.0|M1176932104|           0.0|           0.0|      0|             0|\n",
            "|   1|   DEBIT| 5337.77| C712410124|      41720.0|      36382.23| C195600860|       41898.0|      40348.79|      0|             0|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evzCAkOsnuYg",
        "outputId": "90fac9ed-d953-4bf2-eaa0-27a68f4e5a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6362620"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show(vertical=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXYrBeYroJPM",
        "outputId": "fdcf80af-19c3-460d-ffd5-5724b4d32531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0------------------------------\n",
            " summary        | count                \n",
            " step           | 6362620              \n",
            " type           | 6362620              \n",
            " amount         | 6362620              \n",
            " nameOrig       | 6362620              \n",
            " oldbalanceOrg  | 6362620              \n",
            " newbalanceOrig | 6362620              \n",
            " nameDest       | 6362620              \n",
            " oldbalanceDest | 6362620              \n",
            " newbalanceDest | 6362620              \n",
            " isFraud        | 6362620              \n",
            " isFlaggedFraud | 6362620              \n",
            "-RECORD 1------------------------------\n",
            " summary        | mean                 \n",
            " step           | 243.39724563151657   \n",
            " type           | NULL                 \n",
            " amount         | 179861.90354912292   \n",
            " nameOrig       | NULL                 \n",
            " oldbalanceOrg  | 833883.1040744851    \n",
            " newbalanceOrig | 855113.6685785672    \n",
            " nameDest       | NULL                 \n",
            " oldbalanceDest | 1100701.6665196999   \n",
            " newbalanceDest | 1224996.3982020712   \n",
            " isFraud        | 0.001290820448180152 \n",
            " isFlaggedFraud | 2.51468734577894E-6  \n",
            "-RECORD 2------------------------------\n",
            " summary        | stddev               \n",
            " step           | 142.3319710491244    \n",
            " type           | NULL                 \n",
            " amount         | 603858.2314629285    \n",
            " nameOrig       | NULL                 \n",
            " oldbalanceOrg  | 2888242.6730375285   \n",
            " newbalanceOrig | 2924048.5029542595   \n",
            " nameDest       | NULL                 \n",
            " oldbalanceDest | 3399180.1129945437   \n",
            " newbalanceDest | 3674128.9421197013   \n",
            " isFraud        | 0.035904796801603175 \n",
            " isFlaggedFraud | 0.001585774705736558 \n",
            "-RECORD 3------------------------------\n",
            " summary        | min                  \n",
            " step           | 1                    \n",
            " type           | CASH_IN              \n",
            " amount         | 0.0                  \n",
            " nameOrig       | C1000000639          \n",
            " oldbalanceOrg  | 0.0                  \n",
            " newbalanceOrig | 0.0                  \n",
            " nameDest       | C1000004082          \n",
            " oldbalanceDest | 0.0                  \n",
            " newbalanceDest | 0.0                  \n",
            " isFraud        | 0                    \n",
            " isFlaggedFraud | 0                    \n",
            "-RECORD 4------------------------------\n",
            " summary        | max                  \n",
            " step           | 99                   \n",
            " type           | TRANSFER             \n",
            " amount         | 99999.81             \n",
            " nameOrig       | C999999784           \n",
            " oldbalanceOrg  | 9999991.25           \n",
            " newbalanceOrig | 9999991.25           \n",
            " nameDest       | M999999784           \n",
            " oldbalanceDest | 999999.69            \n",
            " newbalanceDest | 999999.69            \n",
            " isFraud        | 1                    \n",
            " isFlaggedFraud | 1                    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "filtered_data = df.filter(col(\"isFlaggedFraud\") == 1)\n",
        "filtered_data = df.subtract(filtered_data)\n",
        "filtered_data = filtered_data.drop(\"isFlaggedFraud\", \"nameOrig\", \"nameDest\")"
      ],
      "metadata": {
        "id": "D0YGjWIep-Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data.show(5)"
      ],
      "metadata": {
        "id": "IJe8FxE-whbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 1. Data preprocessing\n",
        "# =========================================\n",
        "\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "\n",
        "# Handling missing values\n",
        "filtered_data.dropna()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PJzX7yhY9cL",
        "outputId": "7ed0ac89-c7c9-4c7f-8f68-a3484f6eb5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[step: string, type: string, amount: string, oldbalanceOrg: string, newbalanceOrig: string, oldbalanceDest: string, newbalanceDest: string, isFraud: string]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `cut_df` is our small DataFrame\n",
        "\n",
        "cut_df = filtered_data.sample(fraction=0.005, withReplacement=False, seed=42)"
      ],
      "metadata": {
        "id": "Bof30PON0DxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cut_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auaeF8MfKwj1",
        "outputId": "b26cd689-dad8-4b6f-f949-72e8b09b7aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
            "|step|    type|   amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud|\n",
            "+----+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
            "| 182|CASH_OUT|329225.74|     100093.0|           0.0|    3713483.09|    4042708.83|      0|\n",
            "| 183| CASH_IN|244112.01|   5981354.08|    6225466.09|    1872925.31|     1628813.3|      0|\n",
            "| 184|CASH_OUT| 271104.1|      10228.0|           0.0|           0.0|      271104.1|      0|\n",
            "| 187| PAYMENT| 11882.55|      82728.0|      70845.45|           0.0|           0.0|      0|\n",
            "| 191| CASH_IN| 45594.69|    9192018.6|    9237613.28|     275040.48|     229445.79|      0|\n",
            "| 203| PAYMENT|  1368.59|    241506.58|     240137.99|           0.0|           0.0|      0|\n",
            "| 205| CASH_IN|110934.41|    1082455.8|    1193390.21|    1353901.39|    1242966.98|      0|\n",
            "| 210| PAYMENT|  8995.57|          0.0|           0.0|           0.0|           0.0|      0|\n",
            "| 212| CASH_IN|231899.97|    652758.36|     884658.34|    3278169.45|    3046269.48|      0|\n",
            "| 226|CASH_OUT| 72605.74|      22795.0|           0.0|    5774152.46|     5846758.2|      0|\n",
            "+----+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DoubleType, IntegerType\n",
        "\n",
        "# Cast columns to numeric types\n",
        "numeric_cols = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\"]\n",
        "for col_name in numeric_cols:\n",
        "    cut_df = cut_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
        "\n",
        "filtered_data = filtered_data.withColumn(\"isFraud\", col(\"isFraud\").cast(IntegerType()))"
      ],
      "metadata": {
        "id": "mSMUTezLeZ9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Assuming `data` is your DataFrame and `type` is the column we want to preprocess\n",
        "stringIndexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
        "model = stringIndexer.fit(cut_df)\n",
        "indexed_data = model.transform(cut_df)\n",
        "\n",
        "# Apply OneHotEncoder\n",
        "encoder = OneHotEncoder(inputCols=[\"type_index\"], outputCols=[\"type_encoded\"])\n",
        "encoded_data = encoder.fit(indexed_data).transform(indexed_data)\n",
        "\n",
        "# Drop the original categorical column if desired\n",
        "encoded_data = encoded_data.drop(\"type\", \"type_index\")\n",
        "\n",
        "# Show the preprocessed DataFrame\n",
        "#encoded_data.show()"
      ],
      "metadata": {
        "id": "nObRqm7U201a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble features\n",
        "assembler = VectorAssembler(inputCols=[\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_encoded\"], outputCol=\"features\")\n",
        "scaled_data = assembler.transform(encoded_data)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
        "final_df = scaler.fit(scaled_data).transform(scaled_data)"
      ],
      "metadata": {
        "id": "Jf7Xa7XNaHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9RtpEg8cQ4b",
        "outputId": "2bacc637-69b9-48c8-ee42-8f7386c59320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+-------------+--------------+--------------+--------------+-------+-------------+--------------------+--------------------+\n",
            "|step|   amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud| type_encoded|            features|      scaledFeatures|\n",
            "+----+---------+-------------+--------------+--------------+--------------+-------+-------------+--------------------+--------------------+\n",
            "| 182|329225.74|     100093.0|           0.0|    3713483.09|    4042708.83|    0.0|(4,[0],[1.0])|[329225.74,100093...|[0.49887896717863...|\n",
            "| 183|244112.01|   5981354.08|    6225466.09|    1872925.31|     1628813.3|    0.0|(4,[2],[1.0])|[244112.01,598135...|[0.36990530395558...|\n",
            "| 184| 271104.1|      10228.0|           0.0|           0.0|      271104.1|    0.0|(4,[0],[1.0])|(9,[0,1,4,5],[271...|(9,[0,1,4,5],[0.4...|\n",
            "| 187| 11882.55|      82728.0|      70845.45|           0.0|           0.0|    0.0|(4,[1],[1.0])|(9,[0,1,2,6],[118...|(9,[0,1,2,6],[0.0...|\n",
            "| 191| 45594.69|    9192018.6|    9237613.28|     275040.48|     229445.79|    0.0|(4,[2],[1.0])|[45594.69,9192018...|[0.06909007739197...|\n",
            "+----+---------+-------------+--------------+--------------+--------------+-------+-------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.withColumn(\"isFraud\", col(\"isFraud\").cast(\"integer\"))"
      ],
      "metadata": {
        "id": "3u8vUR_zjaOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 2. Split Data\n",
        "# =========================================\n",
        "\n",
        "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)\n"
      ],
      "metadata": {
        "id": "9ZNaM3bgZJVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 3. Build and Train Models\n",
        "# =========================================\n",
        "\n",
        "# RANDOM FOREST\n",
        "# -------------\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Create a RandomForest model.\n",
        "rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"isFraud\", seed=42)\n",
        "\n",
        "# Train model.\n",
        "model_rf = rf.fit(train_data)\n",
        "\n",
        "# SVM\n",
        "# ---\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "# Create an SVM model.\n",
        "svm = LinearSVC(labelCol=\"isFraud\", featuresCol=\"scaledFeatures\", maxIter=10)\n",
        "\n",
        "# Train model.\n",
        "model_svm = svm.fit(train_data)\n"
      ],
      "metadata": {
        "id": "vukxCCjcZTpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================================\n",
        "# 4. Evaluate the Models\n",
        "# =========================================\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Evaluate models\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"isFraud\")\n",
        "\n",
        "print(\"Random Forest AUC: \", evaluator.evaluate(model_rf.transform(test_data)))\n",
        "print(\"SVM AUC: \", evaluator.evaluate(model_svm.transform(test_data)))\n"
      ],
      "metadata": {
        "id": "adIMXCU2ZhMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12cf110-7eb6-4697-aedd-42e514f6bba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest AUC:  0.9996316334753079\n",
            "SVM AUC:  0.9561989179233338\n"
          ]
        }
      ]
    }
  ]
}